{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "## Text Cleaning & Basic Bag of Words Wizardry\n",
    "\n",
    "* Natalia Zhang wenshuo.zhang@gmail.com\n",
    "* Remember, annotate so that you should be able to find anything with ctrl-F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. Cleaning and processing step-by-step\n",
    "2. Canned Subroutines\n",
    "3. BoW Magic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cleaning Processing Step-by-Step\n",
    "\n",
    "I've modified a real Hostelworld review to illustrate what's happening:\n",
    "\n",
    ">I th1nk the best part    about this hostel was the great social atmosphere and planned activities! It made it really easy to meet and hangout with new people as a solo traveler. The 34 downside however would be the unfinished remodel of the bathroom nd the cleanliness of the dorm room.  But otherwise the location is good, situated in a relatively quiet neighborhood serviced by the northern line and a couple bus routes. The staff were exceptionally friendly and helpful for everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "text = 'I th1nk the best part    about this hostel was the great social atmosphere and planned activities! It made it really easy to meet and hangout with new people as a solo traveler. The 34 downside however would be the unfinished remodel of the bathroom nd the cleanliness of the dorm room.  But otherwise the location is good, situated in a relatively quiet neighborhood serviced by the northern line and a couple bus routes. The staff were exceptionally friendly and helpful for everything.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize me!\n",
    "\n",
    "The next thing is to clean and tokenize the text, though generally \"tokenizing\" involves more than just breaking the text down to word-level vectors.\n",
    "\n",
    "**Steps may include:**\n",
    "1. Lowering word case - Apple is the same thing as apple. Or maybe not!\n",
    "1.5 remove numbers - either independently terms or stuck within words. You decide whether target words are h4x0r, or not.\n",
    "2. Filtering stop words - some words are used a lot, so you may not want them\n",
    "    * You may want to use an existing set or extend/create your own\n",
    "    * a Common question is whether to include negation in the stop words set\n",
    "3. Stemming _or_ lemmatizing\n",
    "    * Stem only cuts the ends of words off\n",
    "    * Lemmatizing is more intelligent and tries to find the word root\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'th1nk', 'the', 'best', 'part', 'about', 'this', 'hostel', 'was', 'the', 'great', 'social', 'atmosphere', 'and', 'planned', 'activities', 'it', 'made', 'it', 'really', 'easy', 'to', 'meet', 'and', 'hangout', 'with', 'new', 'people', 'as', 'a', 'solo', 'traveler', 'the', '34', 'downside', 'however', 'would', 'be', 'the', 'unfinished', 'remodel', 'of', 'the', 'bathroom', 'nd', 'the', 'cleanliness', 'of', 'the', 'dorm', 'room', 'but', 'otherwise', 'the', 'location', 'is', 'good', 'situated', 'in', 'a', 'relatively', 'quiet', 'neighborhood', 'serviced', 'by', 'the', 'northern', 'line', 'and', 'a', 'couple', 'bus', 'routes', 'the', 'staff', 'were', 'exceptionally', 'friendly', 'and', 'helpful', 'for', 'everything']\n"
     ]
    }
   ],
   "source": [
    "import nltk.tokenize\n",
    "\n",
    "#tokenize \n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "\n",
    "#You'd need to do additional processing for lower case\n",
    "tokens = [token.lower() for token in tokens]\n",
    "\n",
    "#also removing punctuation\n",
    "import string\n",
    "tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove numbers, punctuations, stop words\n",
    "\n",
    "The text contains numbers, misspellings, and punctuation in the text, so we'll want to filter it out. \n",
    "\n",
    "However, keep in mind that social media text may be badly misspelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'the', 'best', 'part', 'about', 'this', 'hostel', 'was', 'the', 'great', 'social', 'atmosphere', 'and', 'planned', 'activities', 'it', 'made', 'it', 'really', 'easy', 'to', 'meet', 'and', 'hangout', 'with', 'new', 'people', 'as', 'a', 'solo', 'traveler', 'the', 'downside', 'however', 'would', 'be', 'the', 'unfinished', 'remodel', 'of', 'the', 'bathroom', 'nd', 'the', 'cleanliness', 'of', 'the', 'dorm', 'room', 'but', 'otherwise', 'the', 'location', 'is', 'good', 'situated', 'in', 'a', 'relatively', 'quiet', 'neighborhood', 'serviced', 'by', 'the', 'northern', 'line', 'and', 'a', 'couple', 'bus', 'routes', 'the', 'staff', 'were', 'exceptionally', 'friendly', 'and', 'helpful', 'for', 'everything']\n"
     ]
    }
   ],
   "source": [
    "#remove numerical sequences\n",
    "tokens = [token for token in tokens if not str(token).isnumeric()]\n",
    "\n",
    "#remove tokens that have words containing numbers, eg 7PM, h4x0r\n",
    "#watch out bc this may remove too many words!\n",
    "tokens = [token for token in tokens if not any(char.isdigit() for char in token)]\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove accents\n",
    "\n",
    "Different people may put accents on different words. You may want to clean things up so that fiance and fiancé are regarded as the same word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "àéêöhello   aeeohello\n"
     ]
    }
   ],
   "source": [
    "#Unfortunately NLTK doesn't have a native subroutine for this, so we'll have to write one\n",
    "\n",
    "import unidecode\n",
    "\n",
    "def strip_accent(text):\n",
    "    return unidecode.unidecode(text)\n",
    "    \n",
    "sometext = \"àéêöhello\"\n",
    "print(sometext, \" \", strip_accent(sometext))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# What is the standard NLTK stopwords list?\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['best', 'part', 'hostel', 'great', 'social', 'atmosphere', 'planned', 'activities', 'made', 'really', 'easy', 'meet', 'hangout', 'new', 'people', 'solo', 'traveler', 'downside', 'however', 'would', 'unfinished', 'remodel', 'bathroom', 'nd', 'cleanliness', 'dorm', 'room', 'otherwise', 'location', 'good', 'situated', 'relatively', 'quiet', 'neighborhood', 'serviced', 'northern', 'line', 'couple', 'bus', 'routes', 'staff', 'exceptionally', 'friendly', 'helpful', 'everything']\n"
     ]
    }
   ],
   "source": [
    "#Not that we know, it's a simple list comprehension to remove everything\n",
    "tokens = [ token for token in tokens if token not in stop_words ]\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Endings: Stemming\n",
    "\n",
    "We can either stem OR lemmatize, but lemmatizing is smarter, though it will take slightly more time.\n",
    "\n",
    "There are a bunch of stemmers, and you can choose between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lancaster\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "#set language\n",
    "stemmer = LancasterStemmer()\n",
    "tokens_lancaster = [ stemmer.stem(token) for token in tokens ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#snowball\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#set language\n",
    "stemmer = PorterStemmer()\n",
    "tokens_porter = [ stemmer.stem(token) for token in tokens ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#snowball\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#set language\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "tokens_snowball = [ stemmer.stem(token) for token in tokens ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Lancaster        Porter      Snowball\n",
      "0       best          best          best\n",
      "1       part          part          part\n",
      "2     hostel        hostel        hostel\n",
      "3        gre         great         great\n",
      "4        soc        social        social\n",
      "5    atmosph     atmospher     atmospher\n",
      "6       plan          plan          plan\n",
      "7        act         activ         activ\n",
      "8        mad          made          made\n",
      "9       real        realli        realli\n",
      "10      easy          easi          easi\n",
      "11      meet          meet          meet\n",
      "12   hangout       hangout       hangout\n",
      "13       new           new           new\n",
      "14     peopl         peopl         peopl\n",
      "15      solo          solo          solo\n",
      "16    travel        travel        travel\n",
      "17   downsid       downsid       downsid\n",
      "18     howev         howev         howev\n",
      "19     would         would         would\n",
      "20     unfin      unfinish      unfinish\n",
      "21   remodel       remodel       remodel\n",
      "22  bathroom      bathroom      bathroom\n",
      "23        nd            nd            nd\n",
      "24       cle       cleanli       cleanli\n",
      "25      dorm          dorm          dorm\n",
      "26      room          room          room\n",
      "27    otherw      otherwis      otherwis\n",
      "28       loc         locat         locat\n",
      "29      good          good          good\n",
      "30      situ        situat        situat\n",
      "31       rel           rel         relat\n",
      "32     quiet         quiet         quiet\n",
      "33    neighb  neighborhood  neighborhood\n",
      "34      serv        servic        servic\n",
      "35  northern      northern      northern\n",
      "36       lin          line          line\n",
      "37     coupl         coupl         coupl\n",
      "38       bus            bu           bus\n",
      "39      rout          rout          rout\n",
      "40     staff         staff         staff\n",
      "41    exceiv        except        except\n",
      "42    friend      friendli        friend\n",
      "43      help          help          help\n",
      "44   everyth       everyth       everyth\n"
     ]
    }
   ],
   "source": [
    "compare_stemmers = pd.DataFrame({'Lancaster': tokens_lancaster, 'Porter': tokens_porter, 'Snowball': tokens_snowball})\n",
    "print(compare_stemmers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Endings: Lemmatizing\n",
    "\n",
    "Alternately, you lemmatize for better effect. Depending on the package, it may take a while. Lemmatization usually either takes a word to its adjective or noun roots. In the case below, \"routes\" has been lemmatized to \"route\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Lancaster        Porter      Snowball          Lemma\n",
      "0       best          best          best           best\n",
      "1       part          part          part           part\n",
      "2     hostel        hostel        hostel         hostel\n",
      "3        gre         great         great          great\n",
      "4        soc        social        social         social\n",
      "5    atmosph     atmospher     atmospher     atmosphere\n",
      "6       plan          plan          plan        planned\n",
      "7        act         activ         activ       activity\n",
      "8        mad          made          made           made\n",
      "9       real        realli        realli         really\n",
      "10      easy          easi          easi           easy\n",
      "11      meet          meet          meet           meet\n",
      "12   hangout       hangout       hangout        hangout\n",
      "13       new           new           new            new\n",
      "14     peopl         peopl         peopl         people\n",
      "15      solo          solo          solo           solo\n",
      "16    travel        travel        travel       traveler\n",
      "17   downsid       downsid       downsid       downside\n",
      "18     howev         howev         howev        however\n",
      "19     would         would         would          would\n",
      "20     unfin      unfinish      unfinish     unfinished\n",
      "21   remodel       remodel       remodel        remodel\n",
      "22  bathroom      bathroom      bathroom       bathroom\n",
      "23        nd            nd            nd             nd\n",
      "24       cle       cleanli       cleanli    cleanliness\n",
      "25      dorm          dorm          dorm           dorm\n",
      "26      room          room          room           room\n",
      "27    otherw      otherwis      otherwis      otherwise\n",
      "28       loc         locat         locat       location\n",
      "29      good          good          good           good\n",
      "30      situ        situat        situat       situated\n",
      "31       rel           rel         relat     relatively\n",
      "32     quiet         quiet         quiet          quiet\n",
      "33    neighb  neighborhood  neighborhood   neighborhood\n",
      "34      serv        servic        servic       serviced\n",
      "35  northern      northern      northern       northern\n",
      "36       lin          line          line           line\n",
      "37     coupl         coupl         coupl         couple\n",
      "38       bus            bu           bus            bus\n",
      "39      rout          rout          rout          route\n",
      "40     staff         staff         staff          staff\n",
      "41    exceiv        except        except  exceptionally\n",
      "42    friend      friendli        friend       friendly\n",
      "43      help          help          help        helpful\n",
      "44   everyth       everyth       everyth     everything\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "#let's compare against the stemmed versions earlier\n",
    "compare_stemmers['Lemma'] = tokens\n",
    "print(compare_stemmers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Canned Subroutines\n",
    "\n",
    "Usually I end up combining most of these into a function, or take advantage of preexisting subroutines in different packages. NLTK is the multitool of NLP, but occasionally you may want to use gensim or spaCy.\n",
    "\n",
    "I'm actually lying. You really shouldn't need to use spaCy for anything as basic as text cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK implementation\n",
    "#This is the whole thing crunched together into one function.\n",
    "\n",
    "#It's highly likely that you may not need to do ALL of this cleaning.\n",
    "#Decide what you're trying to do and then judiciously modify subroutines as needed.\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "    \n",
    "def strip_accent(text):\n",
    "    return unidecode.unidecode(text)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    #You may occasionally want to break it into sentences instead with nltk.sentence_tokenize(text)\n",
    "    #and run an extra for loop inside in case you want to create a separate token array for each sentence\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        tokens.append(word)\n",
    "        \n",
    "    \n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    tokens = [token.lower() for token in tokens]   \n",
    "    tokens = [token for token in tokens if not str(token).isnumeric()]\n",
    "    tokens = [token for token in tokens if not any(char.isdigit() for char in token)]\n",
    "    tokens = [ token for token in tokens if token not in stop_words ]\n",
    "    tokens = [ strip_accent(token) for token in tokens ]\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Gensim Simple Preprocess: **\n",
    "\n",
    "0. lower case\n",
    "1. strip punctuation\n",
    "2. removed numbers (and split th1nk into two words on 1 as space)\n",
    "3. remove accents\n",
    "\n",
    "It did not \n",
    "4. lemmatize\n",
    "5. stem\n",
    "\n",
    "Note that there's a minimum and maximum length setting on word size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'th', 'nk', 'the', 'best', 'part', 'about', 'this', 'hostel', 'was', 'the', 'great', 'social', 'atmosphere', 'and', 'planned', 'activities', 'it', 'made', 'it', 'really', 'easy', 'to', 'meet', 'and', 'hangout', 'with', 'new', 'people', 'as', 'a', 'solo', 'traveler', 'the', 'downside', 'however', 'would', 'be', 'the', 'unfinished', 'remodel', 'of', 'the', 'bathroom', 'nd', 'the', 'cleanliness', 'of', 'the', 'dorm', 'room', 'but', 'otherwise', 'the', 'location', 'is', 'good', 'situated', 'in', 'a', 'relatively', 'quiet', 'neighborhood', 'serviced', 'by', 'the', 'northern', 'line', 'and', 'a', 'couple', 'bus', 'routes', 'the', 'staff', 'were', 'exceptionally', 'friendly', 'and', 'helpful', 'for', 'everything']\n"
     ]
    }
   ],
   "source": [
    "#Gensim Pre Process 1\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "g_tokens = simple_preprocess(text, deacc=True, min_len=1, max_len=20)\n",
    "print(g_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Better gensim alternative, preprocess_string: **\n",
    "\n",
    "[Default filters](https://radimrehurek.com/gensim/parsing/preprocessing.html#gensim.parsing.preprocessing.preprocess_string):\n",
    "\n",
    "1. removes HTML tags\n",
    "2. removes punctuation\n",
    "3. removes whitespace\n",
    "4. removes numbers\n",
    "    * unlike simple_preprocess, this one removes the numbers and crunches the word together instead of creating a whitespace, so \"th1nk\" became \"thnk\"\n",
    "5. removes stopwords\n",
    "6. removes words that are less than X number of letters (default = 3)\n",
    "7. stems text using the Porter stemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thnk', 'best', 'hostel', 'great', 'social', 'atmospher', 'plan', 'activ', 'easi', 'meet', 'hangout', 'new', 'peopl', 'solo', 'travel', 'downsid', 'unfinish', 'remodel', 'bathroom', 'cleanli', 'dorm', 'room', 'locat', 'good', 'situat', 'rel', 'quiet', 'neighborhood', 'servic', 'northern', 'line', 'coupl', 'bu', 'rout', 'staff', 'exception', 'friendli', 'help']\n"
     ]
    }
   ],
   "source": [
    "# Alternate method\n",
    "\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "g_tokenss = preprocess_string(text)\n",
    "print(g_tokenss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. BoW Magic\n",
    "\n",
    "Generally speaking, you'll want to transform documents into bag of words (bag-of-words, BoW) with Scikit-Learn, which has its own routines for straight word counts (CountVectorizer) or TFIDF (TFIDF Vectorizer).\n",
    "\n",
    "Both Vectorizers have built-in preprocessing subroutines:\n",
    "0. read from file\n",
    "1. strip accents\n",
    "2. lowercase\n",
    "3. stop_words: built-in or you can call a custom list\n",
    "    *scikit-learn warns that its list is flawed because it's actually incompatible with the built-in tokenizer\n",
    "4. limit word frequency, both at minimum and maximum\n",
    "\n",
    "**Best of all, both vectorizers do ngrams, either at the word or the character level.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer() #runs on default settings\n",
    "\n",
    "#alternately, you can specify everything.\n",
    "#For example, I want to use the tokenizer, tokenize_text that I built above!\n",
    "\n",
    "count_vectorizer = CountVectorizer(tokenizer = tokenize_text)\n",
    "bow_counts = count_vectorizer.fit_transform([text]) #get document counts\n",
    "#SKLearn expects the corpus to be a list, with each document as an item in that list\n",
    "\n",
    "##FULLY CUSTOMIZABLE CODE\n",
    "#count_vectorizer = CountVectorizer(input=’content’, encoding=’utf-8’, decode_error=’strict’, strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern=’(?u)\\b\\w\\w+\\b’, ngram_range=(1, 1), analyzer=’word’, max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class ‘numpy.int64’>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['activity', 'atmosphere', 'bathroom', 'best', 'bus', 'cleanliness', 'couple', 'dorm', 'downside', 'easy', 'everything', 'exceptionally', 'friendly', 'good', 'great', 'hangout', 'helpful', 'hostel', 'however', 'line', 'location', 'made', 'meet', 'nd', 'neighborhood', 'new', 'northern', 'otherwise', 'part', 'people', 'planned', 'quiet', 'really', 'relatively', 'remodel', 'room', 'route', 'serviced', 'situated', 'social', 'solo', 'staff', 'traveler', 'unfinished', 'would']\n"
     ]
    }
   ],
   "source": [
    "print(count_vectorizer.get_feature_names()) #get list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bow_counts.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Word  Count\n",
      "0        activity      1\n",
      "1      atmosphere      1\n",
      "2        bathroom      1\n",
      "3            best      1\n",
      "4             bus      1\n",
      "5     cleanliness      1\n",
      "6          couple      1\n",
      "7            dorm      1\n",
      "8        downside      1\n",
      "9            easy      1\n",
      "10     everything      1\n",
      "11  exceptionally      1\n",
      "12       friendly      1\n",
      "13           good      1\n",
      "14          great      1\n",
      "15        hangout      1\n",
      "16        helpful      1\n",
      "17         hostel      1\n",
      "18        however      1\n",
      "19           line      1\n",
      "20       location      1\n",
      "21           made      1\n",
      "22           meet      1\n",
      "23             nd      1\n",
      "24   neighborhood      1\n",
      "25            new      1\n",
      "26       northern      1\n",
      "27      otherwise      1\n",
      "28           part      1\n",
      "29         people      1\n",
      "30        planned      1\n",
      "31          quiet      1\n",
      "32         really      1\n",
      "33     relatively      1\n",
      "34        remodel      1\n",
      "35           room      1\n",
      "36          route      1\n",
      "37       serviced      1\n",
      "38       situated      1\n",
      "39         social      1\n",
      "40           solo      1\n",
      "41          staff      1\n",
      "42       traveler      1\n",
      "43     unfinished      1\n",
      "44          would      1\n"
     ]
    }
   ],
   "source": [
    "#put them together\n",
    "\n",
    "sparse_bow = pd.DataFrame((count, word) for word, count in zip(bow_counts.toarray().tolist()[0],count_vectorizer.get_feature_names()))\n",
    "sparse_bow.columns = ['Word','Count']\n",
    "print(sparse_bow)\n",
    "\n",
    "#with a bigger corpus, you can also do sparse_bow.sort_values('Count', ascending=False, inplace=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can also try to normalize the word frequency, but frankly you should\n",
    "#probably just do TF-IDF\n",
    "\n",
    "norm_count_vectorizer = CountVectorizer(tokenizer = tokenize_text, norm='l2')\n",
    "normbow_counts = count_vectorizer.fit_transform([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "Most people will use TF-IDF, term frequency-inverse document frequency, where rare words contribute more weights to the model.\n",
    "\n",
    "You can check to see if normalized counts is equal to TF-IDF when the number of documents in a corpus equals 1.\n",
    "\n",
    "General rule of thumb:\n",
    "1. Word importance increases with # occurrences in document\n",
    "2. Word importance decreases with # occurrences in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_vect = TfidfVectorizer(tokenizer=tokenize_text)\n",
    "tfidf_counts = tf_vect.fit_transform([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Word  TF-IDF\n",
      "0        activity       1\n",
      "1      atmosphere       1\n",
      "2        bathroom       1\n",
      "3            best       1\n",
      "4             bus       1\n",
      "5     cleanliness       1\n",
      "6          couple       1\n",
      "7            dorm       1\n",
      "8        downside       1\n",
      "9            easy       1\n",
      "10     everything       1\n",
      "11  exceptionally       1\n",
      "12       friendly       1\n",
      "13           good       1\n",
      "14          great       1\n",
      "15        hangout       1\n",
      "16        helpful       1\n",
      "17         hostel       1\n",
      "18        however       1\n",
      "19           line       1\n",
      "20       location       1\n",
      "21           made       1\n",
      "22           meet       1\n",
      "23             nd       1\n",
      "24   neighborhood       1\n",
      "25            new       1\n",
      "26       northern       1\n",
      "27      otherwise       1\n",
      "28           part       1\n",
      "29         people       1\n",
      "30        planned       1\n",
      "31          quiet       1\n",
      "32         really       1\n",
      "33     relatively       1\n",
      "34        remodel       1\n",
      "35           room       1\n",
      "36          route       1\n",
      "37       serviced       1\n",
      "38       situated       1\n",
      "39         social       1\n",
      "40           solo       1\n",
      "41          staff       1\n",
      "42       traveler       1\n",
      "43     unfinished       1\n",
      "44          would       1\n"
     ]
    }
   ],
   "source": [
    "sparse_tfidf = pd.DataFrame((count, word) for word, count in zip(bow_counts.toarray().tolist()[0],count_vectorizer.get_feature_names()))\n",
    "sparse_tfidf.columns = ['Word','TF-IDF']\n",
    "print(sparse_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also do this in gensim, but Gensim is poorly documented relative to Scikit-learn, and you might as well keep it all within the same package. Also, gensim doesn't seem to have an easy way to do BoW, just TF-IDF.\n",
    "\n",
    "I won't cover the topic further, but here's [the gensim API for TF-IDF](https://radimrehurek.com/gensim/models/tfidfmodel.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
